#+title:Python培训——大数据爬虫篇（市一中--陈一青）
#+data:<2021-07-06 Tue>
#+html_head: <link rel="stylesheet" type="text/css" href="./note//css/worg-classic.css"/>

* 心得
今天的主题“大数据爬虫”是我本人最近刚刚接触，比较陌生，又相当感兴趣的一个话题。所以，非常感谢有这样的一次培训机会，能够接触到当今最流行的比较前沿的技术，也希望今后，类似这样的培训能够多多的举办，让我们信息老师能够快速的提升自我，赶上如今飞速发展的技术。

回到今天的培训上来，上午谷瑞教授的内容相当的细致，用几个简单的案例就把看似神秘复杂的网络爬虫用非常浅显的方式表达了出来。代码的逐行解读，听的非常的过瘾。 通过谷瑞教授的讲解，我了解到所谓的爬虫，其实就是利用python的requests库的get（）方法、post（）方法……，抓取相关网址的网页对象，然后再对网页对象进行处理，从而获取我们需要的数据。代码不长却非常的实用。也让我很轻松的明白了爬虫的基本原理。

今天培训让我收获最大的，可能就是反爬虫的处理和beautiful soup库的使用。 利用chrom浏览器network 数据段中User——Agent（用户代理）数据进行request header的模拟，谷瑞教授，让我们了解到了工具的使用是多么的重要。其实培训中，通过network数据去寻找User-Agent的值，这种方式可能不是最快捷的，才chrom或edge浏览器的地址栏中直接输入“about:version” 来查看User-Agent（用户代理）是最为便捷的。 beautiful soup的使用，让我基本上明白了网络爬虫的本质: 利用规范的html5、css代码中的各种css标签，进行定位，再利用css格式标记和属性截取数据，从而能够批量快速的从网站下载所需数据。这也让我明白了html5和css3中规范各种标记格式的原因。更让我明白了程序员界的一句名言：不要自己造轮子。 这句话的深层次含义。simple is best，python语言本身很简单，但是它的开源，造就了很多的社区，melpa上面有无数个python的库，每一个库也都很简单，不长的代码，只实现一个功能，但是每个库都把简单单一功能做到了最好，并且每个库，都有全球的开发者更新维护。当我们需要某个功能的时候，只需要简单的找到那个最简单的库就可以了，除非你有自信，你能造出比全球程序员合作还牛的轮子。这就是互联网时代、大数据时代的便捷。

最后说一点我个人的一些问题，因为我对emacs的钟爱，几乎离不开emacs，之前在emacs中配置python好像很顺利，但是这次培训，我发现，两个库都有问题，查了很多资料都没有解决，希望借助这次培训，能对python的编译和库的按装使用更多一点的了解。

最后的最后，附上上午培训的笔记。

* 相关笔记
** 目录
   - 大数据爬虫概述
   - 大数据原理
   - requests获取网页数据
   - beautiful soup解析数据
*** 大数据爬虫概述
**** 背景：数据爆炸式增长，人类进入大规模生产、分享和利用数据的时代。
- 数量大
- 多样性
- 速度快
- 价值
**** 应用
**** HTTP协议的基本原理
- 请求/响应模型（B/S)
  发送请求-获取响应内容-解析内容-保存数据
- 两种请求方式
  - get 
  - post
- 名词
  - request method
  - request headers（请求头）
    包含请求的方法、URL、协议版本等信息
  - request URL
  - request Body
**** 网页的构成
由标签组成，每个标签成为一个节点。不同层次的标签组成文本数模型。 他们之间的关系分为兄弟关系、父子关系、以及子孙关系。
**** chrom调出网页源码
  - f12 
  - c-s-i
通过网页源码定位爬取数据位置
**** 网络爬虫概述（spyder）
网络爬虫（spyder） （又称为网页蜘蛛，网络机器人，网页追逐者web crawler）。 是一种按照一定的规则，自动的抓取网页信息的程序或者脚本。
** 大数据爬虫实战
*** 基本原理
- 发送请求获取网站内容并从中提取数据的自动化程序。关键点：
   - 请求
   - 提取
   - 自动化

- 发送请求 ，获取网页代码  
       请求模块 requests库
- 解析网页获取数据
       解析模块 beautiful soup库
*** 安装quests库
- pip install requests
  #+begin_quote
   此处可能会有有点问题，当系统中python2和python3共存时，需要指明pip或是pip3
  #+end_quote
*** quests模块解析get（）方式发送的数据
***** 流程
- 第一步：根据url， 获取网页html 信息（源代码）

- requests 模块实现GET请求，代码如下
#+BEGIN_SRC python
import requests  # 导入rrequests库
#声明
url="http://baidu.com"             # 加载所需网址
response= requests.get(url)        # 连接到url     
statecode= response.status_code    # 返回服务的链接状态
print(statecode)                   # 输出链接状态代码 （200代表正确链接）
response.encoding="uft-8"          # 文本编码设定为utf-8
content= response.text             # 通过text属性获取文本内容
print(content)
#+END_SRC 

***** requests模块获取网页信息(图片)
#+BEGIN_SRC python
import requests
url = "http://"
result= requests.get(url)
imageDate = result.content           # 通过content属性获取二进制文件内容，即imageDate是一个二进制文件
with open("cherry.jpg","wb") as f:   # w代表write（写入），b代表二进制
      f.write(image)

#+END_SRC

***** 反爬虫技术（豆瓣）
伪装头，模拟浏览器,先通过源码，network,查看user-agent（用户代理，chrom核心的浏览器通过地址栏输入“about:version” 来查看User-Agent（用户代理） 
#+BEGIN_SRC  python
imoort requests
headers = {'User-Agent':}      # 设立请求头，User-Agent在 浏览器源码中查看复制
statecode= response.status_code
print(statecode)
response = requests.get(url,headers)   # 得到响应的文本内容
response.encoding="uft-8"     # 文本编码设定为utf-8
content= response.text        # 通过text属性获取文本内容
print(content)

#+END_SRC
*** 解析网页代码
beautiful soup是一个python库，最主要功能是从网页抓取数据后从html或xml文件中格式化数据，从而批量抓取网页标签中的数据。
**** 安装beautiful soup库
#+begin_quote
  pip3 install beautiful soup4
#+end_quote
相关实例：
#+BEGIN_SRC python
from bs4 import BeautifulSoup
import requests
url="http://www.siso.edu.cn"
response=requests.get(url)
response.encoding="utf-8"
content = response.text
# 将从服务器返回的html代码，转化为beautiful支持的lxml格式
bs= BeautifulSoup(content,features="lxml")
result = bs.find_all(class_="news")  # 获取节点
for item in result:
    
# 获取节点中文本
     name = item.a.attrs['href']  # a 标签名， a.attrs[]表示获取标签的属性
     text = name.a.get_text()
print (name, text)

#+END_SRC

beautiful soup中的find和find_all两个检索函数
**** 案例
#+BEGIN_SRC python
import requests
from bs4 import BeautifulSoup
url=""
headers = {'User-Agent':}
response = requests.get(url,headers)
response.encoding = "gbk"
content = response.text
bs = BeautifulSoup(content,"lxml")
list_item = bs.find_all(class_="list-item")

for item in list-item:
      movie_title = item["data-title"]
      public-time = item["data-release"]
      actors = item["data-actors"]
      print(movie_title,",",public_item, actors)
#+END_SRC

* 模块化编程
** 自定义函数
#+BEGIN_SRC 
def 函数名 （参数列表）：
      函数体
#+END_SRC
** 匿名函数lambda
#+BEGIN_SRC 
f = lambda x, y:x + y
f(3,5)
#+END_SRC
