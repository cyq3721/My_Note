#+title:Python 培训
#+data:<2021-07-06 Tue>
#+html_head: <link rel="stylesheet" type="text/css" href="./note/css/worg-classic.css"/>

* Python大数据爬虫技术（主讲：谷瑞）
** 内容目录   
   - 大数据爬虫概述
   - 大数据原理
   - requests获取网页数据
   - beautiful soup解析数据
** 大数据爬虫概述
*** 背景：数据爆炸式增长，人类进入大规模生产、分享和利用数据的时代。
- 数量大
- 多样性
- 速度快
- 价值
*** 应用
*** HTTP协议的基本原理
- 请求/响应模型（B/S)
  发送请求-获取响应内容-解析内容-保存数据
- 两种请求方式
  - get 
  - post
- 名词
  - request method
  - request headers（请求头）
    包含请求的方法、URL、协议版本等信息
  - request URL
  - request Body
*** 网页的构成
由标签组成，每个标签成为一个节点。不同层次的标签组成文本数模型。 他们之间的关系分为兄弟关系、父子关系、以及子孙关系。
*** chrom调出网页源码
  - f12 
  - c-s-i
通过网页源码定位爬取数据位置
** 网络爬虫概述（spyder）
网络爬虫（spyder） （又称为网页蜘蛛，网络机器人，网页追逐者web crawler）。 是一种按照一定的规则，自动的抓取网页信息的程序或者脚本。
*** 基本原理
- 发送请求获取网站内容并从中提取数据的自动化程序。关键点：
   - 请求
   - 提取
   - 自动化

- 发送请求 ，获取网页代码  
       请求模块 requests
- 解析网页  获取数据
       解析模块 beautiful soup
*** quests模块解析get
**** 流程
- 第一步：根据url， 获取网页html 信息（源代码）
- pip install requests 
- requests 模块实现GET请求
#+BEGIN_SRC python
import requests
#声明
url="http://baidu.com"
response= requests.get(rul)
#返回服务的链接状态（200代表正确链接）
statecode= response.status_code
print(statecode)
# 得到响应的文本内容
response.encoding="uft-8"    # 文本编码设定为utf-8
content= response.text  # 通过text属性获取文本内容
print(content)
#+END_SRC 

**** requests模块获取网页信息(图片)
#+BEGIN_SRC python
import requests
url = "http://"
result= requests.get(url)
imageDate = result.content  # 通过content属性获取二进制文件内容，即imageDate是一个二进制文件
with open("cherry.jpg","wb") as f:
      f.write(image)

#+END_SRC

**** 反爬虫技术（豆瓣）
伪装头，模拟浏览器,先通过源码，network,查看user-agent，chrom核心的浏览器通过地址栏输入“about:version” 来查看User-Agent（用户代理） 
#+BEGIN_SRC  python
imoort requests
headers = {'User-Agent':}   #设立请求头，User-Agent在 浏览器源码中查看复制
statecode= response.status_code
print(statecode)
# 得到响应的文本内容
response = requests.get(url,headers)
response.encoding="uft-8"    # 文本编码设定为utf-8
content= response.text  # 通过text属性获取文本内容
print(content)

#+END_SRC
*** 解析网页代码
beautifiul soup是一个python库，最主要功能是从网页抓取数据，可以从html或xml文件中格式化数据

pip3 install beautiful soup4
#+BEGIN_SRC python
from bs4 import BeautifulSoup
import requests
url="http://www.siso.edu.cn"
response=requests.get(url)
response.encoding="utf-8"
content = response.text
# 将从服务器返回的html代码，转化为beautiful支持的lxml格式
bs= BeautifulSoup(content,features="lxml")
result = bs.find_all(class_="news")  # 获取节点
for item in result:
    
# 获取节点中文本
     name = item.a.attrs['href']  # a 啊表示标签名， a.attrs[]表示获取标签的属性

     text = name.a.get_text()
print (name, text)

#+END_SRC

beautiful soup中的find和find_all两个检索函数
*** 案例
#+BEGIN_SRC python
import requests
from bs4 import BeautifulSoup
url=""
headers = {'User-Agent':}
response = requests.get(url,headers)
response.encoding = "gbk"
content = response.text
bs = BeautifulSoup(content,"lxml")
list_item = bs.find_all(class_="list-item")

for item in list-item:
      movie_title = item["data-title"]
      public-time = item["data-release"]
      actors = item["data-actors"]
      print(movie_title,",",public_item, actors)
#+END_SRC
* 新课标 新教材 新教法 （巫雪琴）
** 时代与
